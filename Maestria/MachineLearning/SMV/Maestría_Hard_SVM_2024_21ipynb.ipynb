{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/s110/CollabNotebooks/blob/main/Maestria/MachineLearning/Maestr%C3%ADa_Hard_SVM_2024_21ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Practice  Hard SVM.\n",
    " ----\n",
    "  \n",
    "  University : UTEC \\\\\n",
    "  Course       : Machine Learning \\\\\n",
    "  Professor    : Cristian López Del Alamo \\\\\n",
    "  Topic        : Hard SVM \\\\\n",
    "  Termina      : 12:45\n",
    "   \n",
    "\n",
    " ----\n",
    "\n",
    "Write the names and surnames of the members and the percentage of participation of each one in the development of the practice:\n",
    " - Integrante 1: (%)\n",
    " - Integrante 2: (%)\n",
    " - Integrante 3: (%)\n",
    " - Integrante 4: (%)\n",
    "\n",
    "\n",
    " ----\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ],
   "metadata": {
    "id": "h5URl9pFHUec"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "id": "sit2AQ1GZDju",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.571960Z",
     "start_time": "2025-06-04T03:26:30.506594Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lagrange Multipliers\n",
    "\n",
    "$\\frac{\\partial f(x)}{ \\partial x} = λ \\frac{\\partial g(  x)}{ \\partial x}$\n",
    "\n",
    "----\n",
    "Find the values of  $λ_i$ for each training elements $X_i$.\n",
    "\n",
    "The  ***GetLambda*** function returns a vector that we will call  lambda, such that   $lambda[i]$ will be  $0$, if the element  $X[i]$ does not intersect with any of the lines   $XW^t + b >=1$ o $XW^t + b >=0$\n",
    "\n",
    "Note: X is a matrix, so $X_i$ will be a  $K$-dimensional vector that represent the  i-th  object or  $k$-dimensional point, and  $X_{ij}$ is  the  j-th  element of the  i-th objet.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "- **Note: The code for finding the lambda values is provided to you.**"
   ],
   "metadata": {
    "id": "_-o70Lb1qRVv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def GetLambda(X, y):\n",
    "    n, m = X.shape\n",
    "    y = y.astype(float)\n",
    "    K = np.dot(X, X.T) * np.dot(y, y.T)  # Kernel\n",
    "    P = matrix(K)\n",
    "    q = matrix(-np.ones(n))\n",
    "    G = matrix(-np.eye(n))\n",
    "    h = matrix(np.zeros(n))\n",
    "    A = matrix(y.reshape(1, -1))\n",
    "    b = matrix(np.zeros(1))\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(sol['x'])\n",
    "    return alpha\n",
    "\n",
    "#Ejemplo para utilizar esta función\n",
    "#lamda = GetLambda(X,Y)\n",
    "#sv = lamda > 1e-5\n",
    "#print(sv)"
   ],
   "metadata": {
    "id": "vI6Hn-6UUV1I",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.591153Z",
     "start_time": "2025-06-04T03:26:36.586819Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Calculation of the Weights W\n",
    "$W_j = \\sum_{i=0}^n \\lambda_iy_ix_{ij}$  \n",
    "\n",
    "----\n",
    "Where: $λ_i$ represent  $i-th$ lagrange multiplier, $W_j$ is the $j-th$ weight,   $x_{ij}$ denotes the value of feacture $(j)$ for the $(i)-th$ training objetc, and $y_i$ is the expected output (1 or -1) for the $i-th$ object.\n",
    "\n",
    "$W_j = \\sum_{i=0}^n \\lambda_iy_ix_{ij}$  \n",
    "Note that the summation only includes elements for which the Lagrange\n",
    "\n",
    "----\n",
    "\n",
    "multiplier $lamnda_i$ is nonzero.\n",
    "\n"
   ],
   "metadata": {
    "id": "Lbvs2lvNlmNa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def Get_W(X, Y, lambda_list):\n",
    "  \"\"\"\n",
    "  Calcula el vector de pesos W para un SVM lineal.\n",
    "\n",
    "  Args:\n",
    "    X (np.ndarray): Matriz de datos de entrenamiento, donde cada fila es una\n",
    "                    muestra y cada columna es una característica.\n",
    "                    Dimensiones: (n_samples, n_features).\n",
    "    Y (np.ndarray): Vector de etiquetas de clase (1 o -1) para cada muestra.\n",
    "                    Dimensiones: (n_samples,) o (n_samples, 1).\n",
    "    lambda_list (np.ndarray): Vector de multiplicadores de Lagrange.\n",
    "                              Dimensiones: (n_samples,) o (n_samples, 1).\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: El vector de pesos W. Dimensiones: (n_features,).\n",
    "  \"\"\"\n",
    "  # Asegurarse de que lambda_list y Y sean arrays 1D para cálculos consistentes.\n",
    "  # La salida de GetLambda (sol['x']) es una matriz cvxopt, luego convertida a np.array,\n",
    "  # que probablemente tiene forma (n_samples, 1). Usamos flatten().\n",
    "  # Y también podría ser (n_samples,) o (n_samples, 1).\n",
    "  lambda_flat = lambda_list.flatten()\n",
    "  Y_flat = Y.flatten()\n",
    "\n",
    "  # n_samples, n_features = X.shape # No es estrictamente necesario aquí\n",
    "\n",
    "  # La fórmula es W_j = sum_i (lambda_i * y_i * x_ij)\n",
    "  # Esto se puede vectorizar como W = X^T @ (lambda * y)\n",
    "\n",
    "  # 1. Calcular el término (lambda_i * y_i) para cada muestra.\n",
    "  # Este término pondera la contribución de cada muestra.\n",
    "  # Si lambda_i es cercano a 0 (vector no de soporte), su contribución es mínima.\n",
    "  weighted_terms = lambda_flat * Y_flat\n",
    "\n",
    "  # 2. Calcular W.\n",
    "  # W_j = sum_i (weighted_terms_i * X_ij)\n",
    "  # Esto es equivalente a la transpuesta de X multiplicada por los términos ponderados.\n",
    "  # X.T tiene dimensiones (n_features, n_samples)\n",
    "  # weighted_terms tiene dimensiones (n_samples,)\n",
    "  # El resultado W tendrá dimensiones (n_features,)\n",
    "  W = np.dot(X.T, weighted_terms)\n",
    "\n",
    "  return W"
   ],
   "metadata": {
    "id": "xJwm8DaClJ-f",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.852220Z",
     "start_time": "2025-06-04T03:26:36.849295Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding the Bias (b)\n",
    "\n",
    "$XW^t + b = 0$\n",
    "\n",
    "$b = - \\frac{1}{n}∑_{i=0}^n X_iW^t$\n",
    "\n",
    "Where $X_i$ is a $k$-dimensional vector representing the $i$-th object, and $k$ is the number of features of the object.\n",
    "\n"
   ],
   "metadata": {
    "id": "wctPuU-jnU0Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def Get_b(X, W):\n",
    "  \"\"\"\n",
    "  Calcula el término de sesgo b para un SVM lineal usando la fórmula\n",
    "  b = - (1/n) * sum(X_i @ W).\n",
    "\n",
    "  Args:\n",
    "    X (np.ndarray): Matriz de datos de entrenamiento, donde cada fila es una\n",
    "                    muestra y cada columna es una característica.\n",
    "                    Dimensiones: (n_samples, n_features).\n",
    "    W (np.ndarray): Vector de pesos W. Asumimos que es un array 1D.\n",
    "                    Dimensiones: (n_features,).\n",
    "\n",
    "  Returns:\n",
    "    float: El término de sesgo b.\n",
    "  \"\"\"\n",
    "  # n_samples es el número de filas en X\n",
    "  n_samples = X.shape[0]\n",
    "\n",
    "  # W es un vector 1D (n_features,).\n",
    "  # X es una matriz (n_samples, n_features).\n",
    "  # np.dot(X, W) calcula el producto punto de cada fila de X con W.\n",
    "  # El resultado es un array 1D de longitud n_samples, donde cada\n",
    "  # elemento es X_i @ W^T (o X_i · W).\n",
    "  # Ejemplo:\n",
    "  # X = [[x11, x12], [x21, x22]]\n",
    "  # W = [w1, w2]\n",
    "  # np.dot(X, W) = [x11*w1 + x12*w2, x21*w1 + x22*w2]\n",
    "  xw_products = np.dot(X, W)\n",
    "\n",
    "  # Sumamos todos estos productos punto\n",
    "  sum_xw_products = np.sum(xw_products)\n",
    "\n",
    "  # Calculamos el promedio de los productos punto\n",
    "  mean_xw_products = sum_xw_products / n_samples\n",
    "\n",
    "  # b es el negativo de este promedio\n",
    "  b = -mean_xw_products\n",
    "\n",
    "  return b"
   ],
   "metadata": {
    "id": "IujB29jtnUl7",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.869305Z",
     "start_time": "2025-06-04T03:26:36.865572Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Stage\n",
    "\n",
    "----\n",
    "For this stage, one should only calculate :\n",
    "\n",
    "- $f(X_j) = X_jW^t + b$\n",
    "\n",
    "But since we have already calculated the values of the parameters $W$ and  $b$, then by substituting we have :\n",
    "\n",
    "- $f(X_j) = \\sum_{i=0}^n \\lambda_iy_i<X_{i},X_{j}> + b$\n",
    "\n",
    "Donde: $X_i$ is the i-th  training vector and  $X_j$   is the new vector that passes through the model for predicting the class (1 or -1)\n",
    "\n",
    "Finally, to determine which class the new vector $X_j$   belongs to, it is sufficient to check the sign of f(X_j).\n",
    "\n",
    "  - **If $f(X_j) >=0$ then $Y_j$ = 1 else $Y_j = -1$**\n",
    "  -----"
   ],
   "metadata": {
    "id": "k7L3GAtNoUo7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def Test(X_new, W, b):\n",
    "  \"\"\"\n",
    "  Predice las etiquetas de clase para nuevas muestras utilizando un modelo SVM lineal\n",
    "  previamente entrenado (con W y b conocidos).\n",
    "\n",
    "  Args:\n",
    "    X_new (np.ndarray): Matriz de nuevas muestras a clasificar.\n",
    "                        Cada fila es una muestra, cada columna es una característica.\n",
    "                        Dimensiones: (n_new_samples, n_features).\n",
    "    W (np.ndarray): Vector de pesos del SVM.\n",
    "                    Asumimos que es un array 1D.\n",
    "                    Dimensiones: (n_features,).\n",
    "    b (float): Término de sesgo del SVM.\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: Un vector de etiquetas de clase predichas (1 o -1)\n",
    "                para cada muestra en X_new.\n",
    "                Dimensiones: (n_new_samples,).\n",
    "  \"\"\"\n",
    "  # Calcular la función de decisión para cada muestra en X_new:\n",
    "  # f(X_j) = X_j @ W + b\n",
    "  # np.dot(X_new, W) calcula el producto punto de cada fila de X_new con W.\n",
    "  # Si X_new es (m, k) y W es (k,), el resultado es (m,).\n",
    "  decision_scores = np.dot(X_new, W) + b\n",
    "\n",
    "  # Aplicar la regla de clasificación:\n",
    "  # Si f(X_j) >= 0, entonces Y_j = 1\n",
    "  # Si f(X_j) < 0, entonces Y_j = -1\n",
    "  # La función np.where es ideal para esto.\n",
    "  predictions = np.where(decision_scores >= 0, 1, -1)\n",
    "\n",
    "  return predictions"
   ],
   "metadata": {
    "id": "froBqp3Mp9C5",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:30:11.452419Z",
     "start_time": "2025-06-04T03:30:11.447665Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Database for Testing:\n",
    "[Download](https://docs.google.com/spreadsheets/d/15-E3kiLJ6bCyXuJvSmxYAp2QYMkPX2QlQ597fAsPYy8/edit#gid=0).\n",
    "\n",
    "----\n",
    "Download the database to your disk and use files.upload() to load it onto the drive. The code is provided.\n",
    "----\n",
    "\n",
    "\n",
    "- Split the dataset into 70% for training and 30% for testing.\n",
    "- Add a value of 1 for the first class and -1 for the second class.\n",
    "- In the testing stage, find the number of elements correctly classified and the number of elements incorrectly classified for each class\n",
    "\n",
    "- Create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) which will show us the efficiency of the method.\n",
    "\n",
    "- Do not forget to normalize the data.\n",
    "\n",
    "- Plot the lines that separate both classes.\n",
    "\n",
    "----\n"
   ],
   "metadata": {
    "id": "LslGSJAprlPm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# code for loading  the Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://github.com/s110/CollabNotebooks/raw/refs/heads/main/Maestria/MachineLearning/SMV/DataSet_Iris_2_Clases.csv'\n",
    "try:\n",
    "    data = pd.read_csv(url, encoding='latin1', sep='\\t')\n",
    "    print(\"Archivo leído exitosamente con latin1 y separador punto y coma.\")\n",
    "    print(data.head())\n",
    "except pd.errors.ParserError as pe:\n",
    "    print(f\"ParserError aún con sep=';': {pe}\")\n",
    "    # Puedes probar otros delimitadores comunes si el punto y coma no funciona\n",
    "    # try:\n",
    "    #     df = pd.read_csv(url, encoding='latin1', sep='\\t') # Tabulador\n",
    "    #     print(\"Archivo leído exitosamente con latin1 y separador tabulador.\")\n",
    "    #     print(df.head())\n",
    "    # except pd.errors.ParserError as pe_tab:\n",
    "    #     print(f\"ParserError con sep='\\\\t': {pe_tab}\")\n",
    "    # except Exception as e_other:\n",
    "    #     print(f\"Otro error: {e_other}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error general: {e}\")\n",
    "\n",
    "X = data[[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\"]]\n",
    "Y = data[[\"variety\"]]\n",
    "print(X)\n",
    "\n",
    "# Splitting the dataset into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , random_state=104,  test_size=0.30,    shuffle=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "iwAZ6rT9Wq2R",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:51:08.052169Z",
     "start_time": "2025-06-04T03:51:07.488815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError aún con sep=';': Error tokenizing data. C error: Expected 2 fields in line 4, saw 3\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     22\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError general: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m X = \u001B[43mdata\u001B[49m[[\u001B[33m\"\u001B[39m\u001B[33msepal.length\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33msepal.width\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mpetal.length\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mpetal.width\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m     25\u001B[39m Y = data[[\u001B[33m\"\u001B[39m\u001B[33mvariety\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m     26\u001B[39m \u001B[38;5;28mprint\u001B[39m(X)\n",
      "\u001B[31mNameError\u001B[39m: name 'data' is not defined"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "# Obtaining the values of the parameters W and b, to predict the classes to which the values of X_test belong\n",
    "\n",
    "W =  Get_W(X_test,Y_test,lambda_list)\n",
    "b =  Get_b(X_test,W)\n",
    "\n",
    "# Convert all values greater than 0 to 1, and those less than 0 to -1\n",
    "Y_pred  = np.sign(Test(X_test,W,b))\n",
    "\n",
    "# We create a confution matrix\n",
    "confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [1, -1])\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "8Nmw8Kpxvc-r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "- Subir el link de su colab a canvas\n",
    "- Disfruten aprendiendo. La única forma de aprender es haciendo.\n",
    "- Buena Suerte.\n",
    "----"
   ],
   "metadata": {
    "id": "dGQYlGzEX8u6"
   }
  }
 ]
}
