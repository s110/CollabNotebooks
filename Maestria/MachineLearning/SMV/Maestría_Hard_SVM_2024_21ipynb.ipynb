{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/s110/CollabNotebooks/blob/main/Maestria/MachineLearning/Maestr%C3%ADa_Hard_SVM_2024_21ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Practice  Hard SVM.\n",
    " ----\n",
    "  \n",
    "  University : UTEC \\\\\n",
    "  Course       : Machine Learning \\\\\n",
    "  Professor    : Cristian López Del Alamo \\\\\n",
    "  Topic        : Hard SVM \\\\\n",
    "  Termina      : 12:45\n",
    "   \n",
    "\n",
    " ----\n",
    "\n",
    "Write the names and surnames of the members and the percentage of participation of each one in the development of the practice:\n",
    " - Integrante 1: (%)\n",
    " - Integrante 2: (%)\n",
    " - Integrante 3: (%)\n",
    " - Integrante 4: (%)\n",
    "\n",
    "\n",
    " ----\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ],
   "metadata": {
    "id": "h5URl9pFHUec"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "id": "sit2AQ1GZDju",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.571960Z",
     "start_time": "2025-06-04T03:26:30.506594Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lagrange Multipliers\n",
    "\n",
    "$\\frac{\\partial f(x)}{ \\partial x} = λ \\frac{\\partial g(  x)}{ \\partial x}$\n",
    "\n",
    "----\n",
    "Find the values of  $λ_i$ for each training elements $X_i$.\n",
    "\n",
    "The  ***GetLambda*** function returns a vector that we will call  lambda, such that   $lambda[i]$ will be  $0$, if the element  $X[i]$ does not intersect with any of the lines   $XW^t + b >=1$ o $XW^t + b >=0$\n",
    "\n",
    "Note: X is a matrix, so $X_i$ will be a  $K$-dimensional vector that represent the  i-th  object or  $k$-dimensional point, and  $X_{ij}$ is  the  j-th  element of the  i-th objet.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "- **Note: The code for finding the lambda values is provided to you.**"
   ],
   "metadata": {
    "id": "_-o70Lb1qRVv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def GetLambda(X, y):\n",
    "    n, m = X.shape\n",
    "    y = y.astype(float)\n",
    "    K = np.dot(X, X.T) * np.dot(y, y.T)  # Kernel\n",
    "    P = matrix(K)\n",
    "    q = matrix(-np.ones(n))\n",
    "    G = matrix(-np.eye(n))\n",
    "    h = matrix(np.zeros(n))\n",
    "    A = matrix(y.reshape(1, -1))\n",
    "    b = matrix(np.zeros(1))\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(sol['x'])\n",
    "    return alpha\n",
    "\n",
    "#Ejemplo para utilizar esta función\n",
    "#lamda = GetLambda(X,Y)\n",
    "#sv = lamda > 1e-5\n",
    "#print(sv)"
   ],
   "metadata": {
    "id": "vI6Hn-6UUV1I",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.591153Z",
     "start_time": "2025-06-04T03:26:36.586819Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Calculation of the Weights W\n",
    "$W_j = \\sum_{i=0}^n \\lambda_iy_ix_{ij}$  \n",
    "\n",
    "----\n",
    "Where: $λ_i$ represent  $i-th$ lagrange multiplier, $W_j$ is the $j-th$ weight,   $x_{ij}$ denotes the value of feacture $(j)$ for the $(i)-th$ training objetc, and $y_i$ is the expected output (1 or -1) for the $i-th$ object.\n",
    "\n",
    "$W_j = \\sum_{i=0}^n \\lambda_iy_ix_{ij}$  \n",
    "Note that the summation only includes elements for which the Lagrange\n",
    "\n",
    "----\n",
    "\n",
    "multiplier $lamnda_i$ is nonzero.\n",
    "\n"
   ],
   "metadata": {
    "id": "Lbvs2lvNlmNa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def Get_W(X, Y, lambda_list):\n",
    "  \"\"\"\n",
    "  Calcula el vector de pesos W para un SVM lineal.\n",
    "\n",
    "  Args:\n",
    "    X (np.ndarray): Matriz de datos de entrenamiento, donde cada fila es una\n",
    "                    muestra y cada columna es una característica.\n",
    "                    Dimensiones: (n_samples, n_features).\n",
    "    Y (np.ndarray): Vector de etiquetas de clase (1 o -1) para cada muestra.\n",
    "                    Dimensiones: (n_samples,) o (n_samples, 1).\n",
    "    lambda_list (np.ndarray): Vector de multiplicadores de Lagrange.\n",
    "                              Dimensiones: (n_samples,) o (n_samples, 1).\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: El vector de pesos W. Dimensiones: (n_features,).\n",
    "  \"\"\"\n",
    "  # Asegurarse de que lambda_list y Y sean arrays 1D para cálculos consistentes.\n",
    "  # La salida de GetLambda (sol['x']) es una matriz cvxopt, luego convertida a np.array,\n",
    "  # que probablemente tiene forma (n_samples, 1). Usamos flatten().\n",
    "  # Y también podría ser (n_samples,) o (n_samples, 1).\n",
    "  lambda_flat = lambda_list.flatten()\n",
    "  Y_flat = Y.flatten()\n",
    "\n",
    "  # n_samples, n_features = X.shape # No es estrictamente necesario aquí\n",
    "\n",
    "  # La fórmula es W_j = sum_i (lambda_i * y_i * x_ij)\n",
    "  # Esto se puede vectorizar como W = X^T @ (lambda * y)\n",
    "\n",
    "  # 1. Calcular el término (lambda_i * y_i) para cada muestra.\n",
    "  # Este término pondera la contribución de cada muestra.\n",
    "  # Si lambda_i es cercano a 0 (vector no de soporte), su contribución es mínima.\n",
    "  weighted_terms = lambda_flat * Y_flat\n",
    "\n",
    "  # 2. Calcular W.\n",
    "  # W_j = sum_i (weighted_terms_i * X_ij)\n",
    "  # Esto es equivalente a la transpuesta de X multiplicada por los términos ponderados.\n",
    "  # X.T tiene dimensiones (n_features, n_samples)\n",
    "  # weighted_terms tiene dimensiones (n_samples,)\n",
    "  # El resultado W tendrá dimensiones (n_features,)\n",
    "  W = np.dot(X.T, weighted_terms)\n",
    "\n",
    "  return W"
   ],
   "metadata": {
    "id": "xJwm8DaClJ-f",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.852220Z",
     "start_time": "2025-06-04T03:26:36.849295Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding the Bias (b)\n",
    "\n",
    "$XW^t + b = 0$\n",
    "\n",
    "$b = - \\frac{1}{n}∑_{i=0}^n X_iW^t$\n",
    "\n",
    "Where $X_i$ is a $k$-dimensional vector representing the $i$-th object, and $k$ is the number of features of the object.\n",
    "\n"
   ],
   "metadata": {
    "id": "wctPuU-jnU0Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def Get_b(X, W):\n",
    "  \"\"\"\n",
    "  Calcula el término de sesgo b para un SVM lineal usando la fórmula\n",
    "  b = - (1/n) * sum(X_i @ W).\n",
    "\n",
    "  Args:\n",
    "    X (np.ndarray): Matriz de datos de entrenamiento, donde cada fila es una\n",
    "                    muestra y cada columna es una característica.\n",
    "                    Dimensiones: (n_samples, n_features).\n",
    "    W (np.ndarray): Vector de pesos W. Asumimos que es un array 1D.\n",
    "                    Dimensiones: (n_features,).\n",
    "\n",
    "  Returns:\n",
    "    float: El término de sesgo b.\n",
    "  \"\"\"\n",
    "  # n_samples es el número de filas en X\n",
    "  n_samples = X.shape[0]\n",
    "\n",
    "  # W es un vector 1D (n_features,).\n",
    "  # X es una matriz (n_samples, n_features).\n",
    "  # np.dot(X, W) calcula el producto punto de cada fila de X con W.\n",
    "  # El resultado es un array 1D de longitud n_samples, donde cada\n",
    "  # elemento es X_i @ W^T (o X_i · W).\n",
    "  # Ejemplo:\n",
    "  # X = [[x11, x12], [x21, x22]]\n",
    "  # W = [w1, w2]\n",
    "  # np.dot(X, W) = [x11*w1 + x12*w2, x21*w1 + x22*w2]\n",
    "  xw_products = np.dot(X, W)\n",
    "\n",
    "  # Sumamos todos estos productos punto\n",
    "  sum_xw_products = np.sum(xw_products)\n",
    "\n",
    "  # Calculamos el promedio de los productos punto\n",
    "  mean_xw_products = sum_xw_products / n_samples\n",
    "\n",
    "  # b es el negativo de este promedio\n",
    "  b = -mean_xw_products\n",
    "\n",
    "  return b"
   ],
   "metadata": {
    "id": "IujB29jtnUl7",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:26:36.869305Z",
     "start_time": "2025-06-04T03:26:36.865572Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Stage\n",
    "\n",
    "----\n",
    "For this stage, one should only calculate :\n",
    "\n",
    "- $f(X_j) = X_jW^t + b$\n",
    "\n",
    "But since we have already calculated the values of the parameters $W$ and  $b$, then by substituting we have :\n",
    "\n",
    "- $f(X_j) = \\sum_{i=0}^n \\lambda_iy_i<X_{i},X_{j}> + b$\n",
    "\n",
    "Donde: $X_i$ is the i-th  training vector and  $X_j$   is the new vector that passes through the model for predicting the class (1 or -1)\n",
    "\n",
    "Finally, to determine which class the new vector $X_j$   belongs to, it is sufficient to check the sign of f(X_j).\n",
    "\n",
    "  - **If $f(X_j) >=0$ then $Y_j$ = 1 else $Y_j = -1$**\n",
    "  -----"
   ],
   "metadata": {
    "id": "k7L3GAtNoUo7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def Test(X_new, W, b):\n",
    "  \"\"\"\n",
    "  Predice las etiquetas de clase para nuevas muestras utilizando un modelo SVM lineal\n",
    "  previamente entrenado (con W y b conocidos).\n",
    "\n",
    "  Args:\n",
    "    X_new (np.ndarray): Matriz de nuevas muestras a clasificar.\n",
    "                        Cada fila es una muestra, cada columna es una característica.\n",
    "                        Dimensiones: (n_new_samples, n_features).\n",
    "    W (np.ndarray): Vector de pesos del SVM.\n",
    "                    Asumimos que es un array 1D.\n",
    "                    Dimensiones: (n_features,).\n",
    "    b (float): Término de sesgo del SVM.\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: Un vector de etiquetas de clase predichas (1 o -1)\n",
    "                para cada muestra en X_new.\n",
    "                Dimensiones: (n_new_samples,).\n",
    "  \"\"\"\n",
    "  # Calcular la función de decisión para cada muestra en X_new:\n",
    "  # f(X_j) = X_j @ W + b\n",
    "  # np.dot(X_new, W) calcula el producto punto de cada fila de X_new con W.\n",
    "  # Si X_new es (m, k) y W es (k,), el resultado es (m,).\n",
    "  decision_scores = np.dot(X_new, W) + b\n",
    "\n",
    "  # Aplicar la regla de clasificación:\n",
    "  # Si f(X_j) >= 0, entonces Y_j = 1\n",
    "  # Si f(X_j) < 0, entonces Y_j = -1\n",
    "  # La función np.where es ideal para esto.\n",
    "  predictions = np.where(decision_scores >= 0, 1, -1)\n",
    "\n",
    "  return predictions"
   ],
   "metadata": {
    "id": "froBqp3Mp9C5",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:30:11.452419Z",
     "start_time": "2025-06-04T03:30:11.447665Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Database for Testing:\n",
    "[Download](https://docs.google.com/spreadsheets/d/15-E3kiLJ6bCyXuJvSmxYAp2QYMkPX2QlQ597fAsPYy8/edit#gid=0).\n",
    "\n",
    "----\n",
    "Download the database to your disk and use files.upload() to load it onto the drive. The code is provided.\n",
    "----\n",
    "\n",
    "\n",
    "- Split the dataset into 70% for training and 30% for testing.\n",
    "- Add a value of 1 for the first class and -1 for the second class.\n",
    "- In the testing stage, find the number of elements correctly classified and the number of elements incorrectly classified for each class\n",
    "\n",
    "- Create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) which will show us the efficiency of the method.\n",
    "\n",
    "- Do not forget to normalize the data.\n",
    "\n",
    "- Plot the lines that separate both classes.\n",
    "\n",
    "----\n"
   ],
   "metadata": {
    "id": "LslGSJAprlPm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# code for loading  the Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url='https://github.com/s110/CollabNotebooks/raw/refs/heads/main/Maestria/MachineLearning/SMV/DataSet_Iris_2_Clases.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "X = data[[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\"]]\n",
    "Y = data[[\"variety\"]]\n",
    "print(X)\n",
    "\n",
    "# Splitting the dataset into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , random_state=104,  test_size=0.30,    shuffle=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "iwAZ6rT9Wq2R",
    "ExecuteTime": {
     "end_time": "2025-06-04T03:38:56.579519Z",
     "start_time": "2025-06-04T03:38:55.884184Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 10-11: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeDecodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      5\u001B[39m url=\u001B[33m'\u001B[39m\u001B[33mhttps://github.com/s110/CollabNotebooks/raw/refs/heads/main/Maestria/MachineLearning/SMV/DataSet_Iris_2_Clases.csv\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m data = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m X = data[[\u001B[33m\"\u001B[39m\u001B[33msepal.length\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33msepal.width\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mpetal.length\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mpetal.width\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m      9\u001B[39m Y = data[[\u001B[33m\"\u001B[39m\u001B[33mvariety\u001B[39m\u001B[33m\"\u001B[39m]]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CollabNotebooks\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CollabNotebooks\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CollabNotebooks\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CollabNotebooks\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1895\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1897\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1898\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CollabNotebooks\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001B[39m, in \u001B[36mCParserWrapper.__init__\u001B[39m\u001B[34m(self, src, **kwds)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[33m\"\u001B[39m\u001B[33mdtype_backend\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     91\u001B[39m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[32m     92\u001B[39m     import_optional_dependency(\u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28mself\u001B[39m._reader = \u001B[43mparsers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.unnamed_cols = \u001B[38;5;28mself\u001B[39m._reader.unnamed_cols\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:574\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.__cinit__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:663\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._get_header\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:874\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:891\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:2053\u001B[39m, in \u001B[36mpandas._libs.parsers.raise_parser_error\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:325\u001B[39m, in \u001B[36mdecode\u001B[39m\u001B[34m(self, input, final)\u001B[39m\n",
      "\u001B[31mUnicodeDecodeError\u001B[39m: 'utf-8' codec can't decode bytes in position 10-11: invalid continuation byte"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "# Obtaining the values of the parameters W and b, to predict the classes to which the values of X_test belong\n",
    "\n",
    "W =  Get_W(X_test,Y_test,lambda_list)\n",
    "b =  Get_b(X_test,W)\n",
    "\n",
    "# Convert all values greater than 0 to 1, and those less than 0 to -1\n",
    "Y_pred  = np.sign(Test(X_test,W,b))\n",
    "\n",
    "# We create a confution matrix\n",
    "confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [1, -1])\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "8Nmw8Kpxvc-r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "\n",
    "- Subir el link de su colab a canvas\n",
    "- Disfruten aprendiendo. La única forma de aprender es haciendo.\n",
    "- Buena Suerte.\n",
    "----"
   ],
   "metadata": {
    "id": "dGQYlGzEX8u6"
   }
  }
 ]
}
